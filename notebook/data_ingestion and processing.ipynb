{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8585edec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Logged in as: Faya\n"
     ]
    }
   ],
   "source": [
    "from telethon import TelegramClient\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # loads variables from .env\n",
    "\n",
    "api_id = os.getenv(\"API_ID\")\n",
    "api_hash = os.getenv(\"API_HASH\")\n",
    "session_name = 'ethio_session'\n",
    "phone = '+251954504193'  # remove spaces\n",
    "\n",
    "\n",
    "# === List of e-commerce Telegram channels ===\n",
    "channels = [\n",
    "    'Leyueqa',\n",
    "    'nevacomputer',\n",
    "    'ZemenExpress',\n",
    "    'meneshayeofficial',\n",
    "    'ethio_brand_collection'\n",
    "]\n",
    "\n",
    "# === Directory setup ===\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/clean', exist_ok=True)\n",
    "\n",
    "# === Amharic text preprocessing function ===\n",
    "def preprocess_amharic(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize whitespace\n",
    "    text = text.replace(\"\\u1369\", \"1\")  # Ethiopic digit example\n",
    "    return text\n",
    "\n",
    "# === Fetch messages function ===\n",
    "async def fetch_messages(channel):\n",
    "    channel_entity = await client.get_entity(channel)\n",
    "    messages = []\n",
    "    async for message in client.iter_messages(channel_entity, limit=100):\n",
    "        all_records = []\n",
    "        for channel in channels:\n",
    "            print(f\"Fetching from: {channel}\")\n",
    "            try:\n",
    "                entity = await client.get_entity(channel)\n",
    "                async for msg in client.iter_messages(entity, limit=100):\n",
    "                    record = {\n",
    "                        'channel': channel,\n",
    "                        'message_id': msg.id,\n",
    "                        'date': msg.date.isoformat(),\n",
    "                        'sender_id': msg.sender_id,\n",
    "                        'raw_text': msg.message,\n",
    "                        'clean_text': preprocess_amharic(msg.message),\n",
    "                        'media': None\n",
    "                    }\n",
    "                    # Download media if available\n",
    "                    if msg.media:\n",
    "                        media_path = f\"data/raw/{channel}_{msg.id}\"\n",
    "                        await msg.download_media(media_path)\n",
    "                        record['media'] = media_path\n",
    "                    all_records.append(record)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed for {channel}: {e}\")\n",
    "\n",
    "        # Save to CSV\n",
    "        df = pd.DataFrame(all_records)\n",
    "        df.to_csv(\"data/clean/preprocessed_messages.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "        print(\"âœ… Data ingestion and preprocessing complete.\")\n",
    "client = TelegramClient(session_name, api_id, api_hash)\n",
    "# Define main function\n",
    "async def main():\n",
    "    await client.start(phone=phone)\n",
    "    me = await client.get_me()\n",
    "    print(f\"âœ… Logged in as: {me.first_name}\")\n",
    "    await client.disconnect()\n",
    "# === Run it ===\n",
    "if __name__ == \"__main__\":\n",
    "    await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fca4a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching from: Leyueqa\n",
      "Fetching from: nevacomputer\n",
      "Fetching from: ZemenExpress\n",
      "Fetching from: meneshayeofficial\n",
      "Fetching from: ethio_brand_collection\n",
      "Saved CSV and JSON with text, phones, and prices.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_phone_numbers(text):\n",
    "    if not text:\n",
    "        return []\n",
    "    pattern = r'\\+?\\d[\\d\\s\\-]{7,}\\d'\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "def extract_prices(text):\n",
    "    if not text:\n",
    "        return []\n",
    "    pattern = r'\\b\\d{1,6}(\\.\\d{1,2})?\\s?(birr|etb|â‚¨)?\\b'\n",
    "    # Returns list of tuples, take first element of each\n",
    "    matches = re.findall(pattern, text.lower())\n",
    "    return [m[0] for m in matches]\n",
    "\n",
    "async def fetch_messages(channel):\n",
    "    print(f\"Fetching from: {channel}\")\n",
    "    all_records = []\n",
    "    try:\n",
    "        entity = await client.get_entity(channel)\n",
    "        async for msg in client.iter_messages(entity, limit=100):\n",
    "            raw_text = msg.message or \"\"\n",
    "            phones = extract_phone_numbers(raw_text)\n",
    "            prices = extract_prices(raw_text)\n",
    "            record = {\n",
    "                'channel': channel,\n",
    "                'message_id': msg.id,\n",
    "                'date': msg.date.isoformat(),\n",
    "                'text': raw_text,\n",
    "                'phone_numbers': phones,\n",
    "                'prices': prices,\n",
    "            }\n",
    "            all_records.append(record)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed for {channel}: {e}\")\n",
    "    return all_records\n",
    "\n",
    "async def main():\n",
    "    await client.start(phone=phone)\n",
    "    all_data = []\n",
    "    for channel in channels:\n",
    "        records = await fetch_messages(channel)\n",
    "        all_data.extend(records)\n",
    "\n",
    "    # Save as CSV\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df.to_csv(\"data/clean/messages_with_contacts_and_prices.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Save as JSON\n",
    "    with open(\"data/clean/messages_with_contacts_and_prices.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"Saved CSV and JSON with text, phones, and prices.\")\n",
    "    await client.disconnect()\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46680fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scheduler started. Waiting for scheduled time...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 76\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     75\u001b[39m     schedule.run_pending()\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     time.sleep(\u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from telethon import TelegramClient, events\n",
    "import os\n",
    "import asyncio\n",
    "import schedule\n",
    "import time\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # loads variables from .env\n",
    "\n",
    "API_ID = os.getenv(\"API_ID\")\n",
    "API_HASH = os.getenv(\"API_HASH\")\n",
    "\n",
    "session_name = 'ethio_session'\n",
    "phone = '+251954504193'\n",
    "\n",
    "channels = [\n",
    "    'Leyueqa',\n",
    "    'nevacomputer',\n",
    "    'ZemenExpress',\n",
    "    'meneshayeofficial',\n",
    "    'ethio_brand_collection'\n",
    "]\n",
    "\n",
    "os.makedirs('data/real_time/raw', exist_ok=True)\n",
    "os.makedirs('data/clean', exist_ok=True)\n",
    "\n",
    "client = TelegramClient(session_name, api_id, api_hash)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return text.strip()\n",
    "\n",
    "@client.on(events.NewMessage(chats=channels))\n",
    "async def handler(event):\n",
    "    msg = event.message\n",
    "    data = {\n",
    "        'channel': event.chat.username if event.chat else None,\n",
    "        'message_id': msg.id,\n",
    "        'date': msg.date.isoformat(),\n",
    "        'sender_id': msg.sender_id,\n",
    "        'text': preprocess_text(msg.message),\n",
    "        'media': None\n",
    "    }\n",
    "\n",
    "    if msg.media:\n",
    "        media_path = f\"data/real_time/raw/{data['channel']}_{msg.id}\"\n",
    "        await msg.download_media(media_path)\n",
    "        data['media'] = media_path\n",
    "\n",
    "    df = pd.DataFrame([data])\n",
    "    csv_path = \"data/clean/messages_with_contacts_and_prices.csv\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    else:\n",
    "        df.to_csv(csv_path, mode='a', header=False, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"New message collected from {data['channel']}: {data['text'][:30]}...\")\n",
    "\n",
    "async def main():\n",
    "    await client.start(phone=phone)\n",
    "    print(\"Listening for new messages...\")\n",
    "    await client.run_until_disconnected()\n",
    "\n",
    "def run_listener():\n",
    "    print(\"Starting listener at scheduled time...\")\n",
    "    asyncio.run(main())  # Properly run async main in sync function\n",
    "\n",
    "# Schedule to run every day at 08:00 AM\n",
    "schedule.every().day.at(\"08:00\").do(run_listener)\n",
    "\n",
    "print(\"Scheduler started. Waiting for scheduled time...\")\n",
    "\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7022f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"data/clean/messages_with_contacts_and_prices.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "def preprocess_amharic(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    # Remove punctuation (you can customize for Amharic punctuation)\n",
    "    text = re.sub(r\"[!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~]\", \" \", text)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    # Lowercase (Amharic script doesnâ€™t have case, but safe for mixed text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Optional: Normalize Amharic-specific chars (example: replace áŠ  and áŠ£ to one form)\n",
    "    # text = text.replace('áŠ ', 'áŠ£')  # example replacement, adjust as needed\n",
    "\n",
    "    # Tokenize - using nltk (download punkt tokenizer before running)\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing and create new column\n",
    "df['tokens'] = df['text'].apply(preprocess_amharic)\n",
    "\n",
    "# Example: Save back to CSV or use df['tokens'] for further analysis\n",
    "df.to_csv(\"data/clean/messages_preprocessed.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "008cb68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 1 tokens: ['ğŸ”¥ğŸ”¥saachi', 'áŠ áŠ•áŒ€á‰µ', 'áŠ áˆ­áˆµ', 'áˆµá‰¶á‰­', 'á‰£áˆˆ', '2', 'á‰°á‰½', 'áˆµá‰¶á‰­', 'ğŸ”„an', 'environmentally', 'friendly', 'and', 'safe', 'plate', 'based', 'on', 'a', 'light', 'wave', 'ğŸ’¯light', 'wave', 'technology', 'ğŸ’¯safe', 'and', 'high', 'quality', 'â›”ï¸does', 'not', 'emit', 'smoke', 'and', 'flame', 'suitable', 'for', 'this', 'product', 'cooking', 'ğŸ™‚can', 'be', 'roasted', 'and', 'steamed', 'this', 'appliance', 'can', 'perform', 'a', 'variety', 'of', 'functions', 'including', 'a', 'multi', 'function', 'plate', 'has', 'a', 'heating', 'function', 'ğŸŸ¢áŒá‹œá‹áŠ•', 'áŠ¥áŠ“', 'áŒ‰áˆá‰ á‰µá‹áŠ•', 'á‹¨áˆšá‰†áŒ¥á‰¥', 'áˆáŒ£áŠ•', 'áˆµá‰¶á‰­', 'ğŸŸ¢á‰£áˆˆ', '2', 'price', '9500', 'delivery', 'free', 'áŠ­áá‹«á‹áŠ•', 'á‹•á‰ƒá‹‰', 'áŠ¥áŒ…á‹', 'áˆ²á‹°áˆ­áˆµ', 'á‰ áˆá‰£á‹­áˆ', 'á‰£áŠ•áŠªáŠ•áŒ', 'áˆ˜áˆá€áˆ', 'á‹­á‰½áˆ‹áˆ‰á‰ á‰°áŒ¨áˆ›áˆª', 'áˆáˆˆá‰µ', 'á‹•á‰ƒá‹á‰½áŠ•', 'áŠ¨', '1000', 'á‰¥áˆ­', 'á‰ áˆ‹á‹­', 'á‹¨áˆšá‰°áˆ˜áŠ‘', '2', 'á‹•á‰ƒá‹á‰½áŠ•', 'áŠ áŠ•á‹´', 'áˆ²áŒˆá‹™', 'áˆµáŒ¦á‰³', 'áŠ¥áŠ•áˆáŠ­áˆˆá‹á‰³áˆˆáŠ•', '0933334444', 'lemazez', 'z', '0944109295', 'lemaze', 'z', '0946242424', 'le', 'mazez']\n",
      "Record 2 tokens: ['ğŸ‘‰ğŸ‘‰ğŸ‘‰super', 'stretch', 'silicon', 'lids', '6', 'pack', 'ğŸ”°á‹¨á‹•á‰ƒ', 'áˆ˜áˆ¸áˆáŠ›', 'áˆ²áˆŠáŠ¨áŠ•', 'á•áˆ‹áˆµá‰²áŠ®á‰½', 'á‰ á‰°á‹°áŒ‹áŒ‹áˆš', 'á‹¨áˆáŠ•áŒ á‰€áˆá‰£á‰¸á‹‰', 'á‰ á‰°áˆˆá‹«á‹©', 'áˆ³á‹­á‹á‰½', 'á‹¨á‰€áˆ¨á‰¡', 'á‹¨áˆšáˆˆáŒ áŒ¥', 'á‹¨áˆ›á‹­á‰€á‹°á‹µ', 'kitchen', 'home', 'silicone', 'stretch', 'lids', 'set', 'of', '6', 'silicone', 'food', 'saver', 'covers', 'ğŸ”°', 'superior', 'quality', 'ğŸ”°', 'lids', 'that', 'fit', 'ğŸ”°', 'reusable', 'ğŸ”°', 'temperature', 'resistant', 'ğŸ”°', 'food', 'safe', 'á‹‹áŒ‹ğŸ‘‰', '450á‰¥áˆ­', 'á‰ á‰°áŒ¨áˆ›áˆª', 'áŠ¨', '1000', 'á‰¥áˆ­', 'á‰ áˆ‹á‹­', 'á‹¨áˆ†áŠ‘', 'áˆáˆˆá‰µ', 'á‹•á‰ƒá‹á‰½', 'á‰ áŠ áŠ•á‹µá‹²áˆŠá‰¨áˆª', 'áŒá‹œ', 'áˆ²á‹«á‹™', 'áˆµáŒ¦á‰³', 'áŠ¥áŠ•áˆáŠ­áˆˆá‹á‰³áˆˆáŠ•', 'ğŸ', 't', 'me', 'leyueqa', 'ğŸ‘ˆá‰»áŠ“áˆ‹á‰½áŠ•áŠ•', 'áˆˆáŒ“á‹°áŠ›á‹', 'áˆ¸áˆ­', 'áˆ›á‹µáˆ¨áŒá‹áŠ•', 'áŠ á‹­áˆ­áˆ±', '0933334444', 'lemazez', 'z', '0944109295', 'lemaze', 'z', '0946242424', 'le', 'mazez']\n",
      "Record 3 tokens: ['ğŸ”ğŸ”ğŸ”saachi', '3', 'in', '1', 'original', 'á‹¨áŒáˆµ', 'á‹¨á‰¡áŠ“', 'á‹¨á‰…áˆ˜áˆ', 'â•1', '6', 'litre', 'high', 'capacity', 'plastic', 'jar', 'â•2', 'speed', 'control', 'with', 'pulse', 'â•full', 'safety', 'switch', 'âš¡ï¸powerful', 'motor', 'â­ï¸ultra', 'sharp', 'blade', 'âœ…rotary', 'switch', 'for', 'easy', 'operate', 'ğŸ”»plastic', 'lined', 'grinding', 'mill', 'ğŸ’µá‹‹áŒ‹', '2900', 'áŠ¨áŠáƒ', 'á‹²áˆŠá‰¨áˆª', 'áŒ‹áˆ­', 'ğŸ’µ', 'ğŸ”–áŠ á‹µáˆ«áˆ»', 'ğŸ¥‡á‰áŒ¥áˆ­', '1', 'áˆá‹°á‰³', 'á‹ˆá‹°', 'á‰£áˆá‰»', 'áˆ†áˆµá’á‰³áˆ', 'áŒˆá‰£', 'á‰¥áˆ', 'áŠ áˆ…áˆ˜á‹µ', 'áˆ…áŠ•áƒ', 'áˆ‹á‹­', '1áŠ›áá‰…', '114b', 'á‰£áˆ‰á‰ á‰µ', 'á‹«áˆˆá‰°áŒ¨áˆ›áˆª', 'áŠ­áá‹«', 'áˆ›á‹˜á‹', 'á‹­á‰½áˆ‹áˆ‰', '0933334444', 'lemazez', 'z', '0946242424', 'le', 'mazez', '0944109295', 'lemaze', 'z']\n",
      "Record 4 tokens: []\n",
      "Record 5 tokens: []\n",
      "Record 6 tokens: []\n",
      "Record 7 tokens: []\n",
      "Record 8 tokens: ['ğŸ”ğŸ”ğŸ”ğŸ”bamboo', 'trey', '3pc', 'âœ…', 'priceâœ…', '3300', 'áŠ¨áŠáƒ', 'á‹²áˆŠá‰¨áˆª', 'áŒ‹áˆ­', '0933334444', 'lemazez', 'z', '0946242424', 'le', 'mazez', '0944109295', 'lemaze', 'z', 'áŠ­áá‹«á‹áŠ•', 'á‰ áˆá‰£á‹­áˆ', 'á‰£áŠ•áŠªáŠ•áŒ', 'áˆ˜áˆá€áˆáˆ', 'á‹­á‰½áˆ‹áˆ‰', 'á‰ á‰°áŒ¨áˆ›áˆª', 'áŠ¨', '1000', 'á‰¥áˆ­', 'á‰ áˆ‹á‹­', 'á‹¨áˆ†áŠ‘', 'áˆáˆˆá‰µ', 'á‹•á‰ƒá‹á‰½', 'áˆ²áŒˆá‹™', 'áˆµáŒ¦á‰³', 'áŠ¥áŠ•áˆáŠ­áˆˆá‹á‰³áˆˆáŠ•', 'ğŸ', 't', 'me', 'leyueqa', 'ğŸ‘ˆá‰»áŠ“áˆ‹á‰½áŠ•áŠ•', 'áˆˆáŒ“á‹°áŠ›á‹', 'áˆ¸áˆ­', 'áˆ›á‹µáˆ¨áŒá‹áŠ•', 'áŠ á‹­áˆ­áˆ±']\n",
      "Record 9 tokens: ['ğŸ›¡ğŸ›¡ğŸ›¡á‹‰áˆƒ', 'áˆµáˆ­áŒˆá‰µáŠ•', 'á‹ˆá‹°', 'ááˆ«áˆ½', 'á‹‰áˆµáŒ¥', 'áŠ¥áŠ•á‹³á‹­áŒˆá‰£', 'áŠ¥áŠ“', 'áŠ áˆ‹áˆµáˆáˆ‹áŒŠ', 'áˆ½á‰³áŠ•', 'áŠ¥áŠ•á‹²áˆáˆ', 'á‹µáŠ«áˆáŠ•', 'á‹¨áˆšáŠ¨áˆ‹áŠ¨áˆ', 'áŠ áŠ•áˆ¶áˆ‹', 'mattress', 'protector', 'polyester', 'microfiber', 'âœ”ï¸áˆˆ', '1', '50', 'âœ”ï¸áˆˆ', '1', '80', 'áŠ¨áŠáƒ', 'á‹²áˆŠá‰¨áˆª', 'áŒ‹áˆ­', '3500', 'á‰¥áˆ­', 'ğŸ’µğŸ’µ', 'ğŸ“ŒğŸ“ŒğŸ“ŒáŠ á‹µáˆ«áˆ»', 'â­•ï¸á‰áŒ¥áˆ­', '1', 'áˆá‹°á‰³', 'á‹ˆá‹°', 'á‰£áˆá‰»', 'áˆ†áˆµá’á‰³áˆ', 'áŒˆá‰£', 'á‰¥áˆ', 'áŠ áˆ…áˆ˜á‹µ', 'áˆ…áŠ•áƒ', 'áˆ‹á‹­', '1áŠ›áá‰…', '114b', 'á‰£áˆ‰á‰ á‰µ', 'á‹«áˆˆá‰°áŒ¨áˆ›áˆª', 'áŠ­áá‹«', 'áˆ›á‹˜á‹', 'á‹­á‰½áˆ‹áˆ‰', '0933334444', 'lemazez', 'z', '0946242424', 'le', 'mazez', '0944109295', 'lemaze', 'z']\n",
      "Record 10 tokens: ['ğŸ’™ğŸ’™ğŸ’™ğŸ’™ğŸ’™ğŸ’™ğŸ’™', 'kitchen', 'oil', 'proof', 'stickers', 'áˆˆáŠªá‰½áŠ•á‹', 'áŒáˆ­áˆ›áˆáŒˆáˆµ', 'á‹¨áˆšá‹«áˆ‹á‰¥áˆµ', 'áŠªá‰½áŠ•á‹áŠ•', 'á…á‹µá‰µ', 'á‹¨áˆšá‹«á‹°áˆ­áŒ', 'â­ï¸áˆˆáˆ˜áˆ³á‰¢á‹«', 'á£', 'áˆˆáŠ«á‰¢áŠ”', 'á£', 'áˆˆáŠ©áˆ½áŠ“', 'á£', 'áˆˆáˆµáˆ‹áˆ³', 'á‹¨áŠ¥áŠ•áŒ¨á‰µ', 'áŒˆáŒ½áŠ“', 'áŒá‹µáŒá‹³', 'áˆ‹á‹­', 'á‹¨áˆšáˆˆáŒ á', 'á‰ áˆ«áˆ±', 'á‹¨áˆšáŒ£á‰ á‰…', 'á‹áˆƒ', 'á‹¨áˆ›á‹«áˆ³áˆá', 'waterproof', 'á‹¨áˆ†áŠ', 'â­ï¸áˆµá‹á‰µ', '60cmÃ—5meter', 'áˆœá‰µáˆ­', 'á‹‹áŒ‹', '1200', 'á‰¥áˆ­', 'áˆ²á‹«á‹™', 'áŠáƒ', 'á‹²áˆŠá‰¨áˆª', 'â­ï¸áˆ™á‰€á‰µáŠ•áŠ¥áŠ“', 'áŠ¥áˆ­áŒ¥á‰ á‰µáŠ•', 'á‹¨áˆšá‰‹á‰‹áˆ', 'áˆˆáˆ›á…á‹³á‰µ', 'á‰€áˆ‹áˆ', '0933334444', 'lemazez', 'z', '0944109295', 'lemaze', 'z', '0946242424', 'le', 'mazez', 'ğŸ”ºáŠ á‹µáˆ«áˆ»', 'â­•ï¸á‰áŒ¥áˆ­', '1', 'áˆá‹°á‰³', 'á‹ˆá‹°', 'á‰£áˆá‰»', 'áˆ†áˆµá’á‰³áˆ', 'áŒˆá‰£', 'á‰¥áˆ', 'áŠ áˆ…áˆ˜á‹µ', 'áˆ…áŠ•áƒ', 'áˆ‹á‹­', '1áŠ›áá‰…', '114b']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the preprocessed CSV with the tokens column\n",
    "df = pd.read_csv(\"data/clean/messages_preprocessed.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# The 'tokens' column contains lists stored as strings, so convert them back to lists\n",
    "import ast\n",
    "\n",
    "df['tokens'] = df['tokens'].apply(ast.literal_eval)\n",
    "\n",
    "# Now print the first 10 rows of tokens\n",
    "for i, tokens in enumerate(df['tokens'].head(10)):\n",
    "    print(f\"Record {i+1} tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1043bdcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel</th>\n",
       "      <th>message_id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>phone_numbers</th>\n",
       "      <th>prices</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>nevacomputer</td>\n",
       "      <td>8733</td>\n",
       "      <td>2025-03-20T12:43:37+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>nevacomputer</td>\n",
       "      <td>8774</td>\n",
       "      <td>2025-06-11T13:56:52+00:00</td>\n",
       "      <td>LENOVO X1 YOGA\\nProcessor: 11thâ€‘Gen Intel Core...</td>\n",
       "      <td>['+251912759900', '+251920153333']</td>\n",
       "      <td>['', '', '.8', '.7', '', '', '', '', '', '', '...</td>\n",
       "      <td>[lenovo, x1, yoga, processor, 11thâ€‘gen, intel,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          channel  message_id                       date  \\\n",
       "139  nevacomputer        8733  2025-03-20T12:43:37+00:00   \n",
       "101  nevacomputer        8774  2025-06-11T13:56:52+00:00   \n",
       "\n",
       "                                                  text  \\\n",
       "139                                                NaN   \n",
       "101  LENOVO X1 YOGA\\nProcessor: 11thâ€‘Gen Intel Core...   \n",
       "\n",
       "                          phone_numbers  \\\n",
       "139                                  []   \n",
       "101  ['+251912759900', '+251920153333']   \n",
       "\n",
       "                                                prices  \\\n",
       "139                                                 []   \n",
       "101  ['', '', '.8', '.7', '', '', '', '', '', '', '...   \n",
       "\n",
       "                                                tokens  \n",
       "139                                                 []  \n",
       "101  [lenovo, x1, yoga, processor, 11thâ€‘gen, intel,...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(2)  # Random 10 rows (good for getting a feel of the dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83c006f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Missing/Nan values in 'tokens' column: 0\n",
      "Empty token lists: 190\n"
     ]
    }
   ],
   "source": [
    "missing_tokens_count = df['tokens'].isna().sum()\n",
    "print(f\"ğŸ” Missing/Nan values in 'tokens' column: {missing_tokens_count}\")\n",
    "empty_tokens_count = df['tokens'].apply(lambda x: isinstance(x, list) and len(x) == 0).sum()\n",
    "print(f\"Empty token lists: {empty_tokens_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61bae447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cleaned DataFrame: 310 rows remaining\n"
     ]
    }
   ],
   "source": [
    "df = df[df['tokens'].apply(lambda x: len(x) > 0 if isinstance(x, list) else False)].reset_index(drop=True)\n",
    "df.to_csv(\"data/clean/structured_telegram_messages_cleaned.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"âœ… Cleaned DataFrame: {len(df)} rows remaining\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e32645a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Structured data saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "\n",
    "# Load raw message CSV (you can also load from JSON and convert)\n",
    "df = pd.read_csv(\"data/clean//structured_telegram_messages_cleaned.csv\", encoding=\"utf-8-sig\")\n",
    "\n",
    "# If tokens are saved as strings, convert to actual list\n",
    "if df['tokens'].dtype == object and isinstance(df['tokens'].iloc[0], str):\n",
    "    df['tokens'] = df['tokens'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# Clean the text content\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"[\\\"#$%&'()*+/<=>@[\\]^_`{|}~á£á¢]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Extract tokens (simple whitespace-based for Amharic)\n",
    "df['tokens'] = df['clean_text'].apply(lambda x: x.split())\n",
    "\n",
    "# Extract flag if the message contains phone number or price\n",
    "def contains_phone(text):\n",
    "    return bool(re.search(r\"\\b09\\d{8}\\b\", text))\n",
    "\n",
    "def contains_price(text):\n",
    "    return bool(re.search(r\"(?:birr|á‰¥áˆ­|\\b\\d{2,7}\\b)\", text, re.IGNORECASE))\n",
    "\n",
    "df['contains_phone'] = df['text'].apply(contains_phone)\n",
    "df['contains_price'] = df['text'].apply(contains_price)\n",
    "\n",
    "# Optional: Create a 'content' column for easier reference\n",
    "df['content'] = df['clean_text']\n",
    "\n",
    "# Reorder and structure columns\n",
    "metadata_cols = ['channel', 'message_id', 'date']\n",
    "content_cols = ['text', 'clean_text', 'tokens', 'phone_numbers', 'prices']\n",
    "\n",
    "df_structured = pd.DataFrame()\n",
    "df_structured[metadata_cols] = df[metadata_cols]\n",
    "df_structured['raw_text'] = df['text']\n",
    "df_structured['clean_text'] = df['clean_text']\n",
    "df_structured['tokens'] = df['tokens']\n",
    "df_structured['contains_phone'] = df['contains_phone']\n",
    "df_structured['contains_price'] = df['contains_price']\n",
    "\n",
    "# Save to CSV\n",
    "df_structured.to_csv(\"data/clean/structured_telegram_messages.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"âœ… Structured data saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b0c974b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel</th>\n",
       "      <th>message_id</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>phone_numbers</th>\n",
       "      <th>prices</th>\n",
       "      <th>tokens</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>contains_phone</th>\n",
       "      <th>contains_price</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>ethio_brand_collection</td>\n",
       "      <td>6081</td>\n",
       "      <td>2025-05-17T05:44:59+00:00</td>\n",
       "      <td>Skechers Quantum flex \\nsize 40,41,42,43\\nPric...</td>\n",
       "      <td>['0920238243']</td>\n",
       "      <td>['', '', '', '', '', '']</td>\n",
       "      <td>[Skechers, Quantum, flex, size, 40,41,42,43, P...</td>\n",
       "      <td>Skechers Quantum flex size 40,41,42,43 Price 3...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Skechers Quantum flex size 40,41,42,43 Price 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>meneshayeofficial</td>\n",
       "      <td>951</td>\n",
       "      <td>2025-03-02T11:19:30+00:00</td>\n",
       "      <td>áˆˆá‰ áˆˆáŒ  áˆ˜áˆ¨áŒƒ áŠ¨á‰³á‰½ á‹«áˆˆá‹áŠ• áˆ›áˆµáˆáŠ•áŒ áˆªá‹« á‹­áŒ«áŠ‘\\n\\nhttps://menes...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[áˆˆá‰ áˆˆáŒ , áˆ˜áˆ¨áŒƒ, áŠ¨á‰³á‰½, á‹«áˆˆá‹áŠ•, áˆ›áˆµáˆáŠ•áŒ áˆªá‹«, á‹­áŒ«áŠ‘]</td>\n",
       "      <td>áˆˆá‰ áˆˆáŒ  áˆ˜áˆ¨áŒƒ áŠ¨á‰³á‰½ á‹«áˆˆá‹áŠ• áˆ›áˆµáˆáŠ•áŒ áˆªá‹« á‹­áŒ«áŠ‘</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>áˆˆá‰ áˆˆáŒ  áˆ˜áˆ¨áŒƒ áŠ¨á‰³á‰½ á‹«áˆˆá‹áŠ• áˆ›áˆµáˆáŠ•áŒ áˆªá‹« á‹­áŒ«áŠ‘</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>ethio_brand_collection</td>\n",
       "      <td>6090</td>\n",
       "      <td>2025-05-28T09:05:44+00:00</td>\n",
       "      <td>Skechers Out door taupe \\nSize 40,41,42,43\\nPr...</td>\n",
       "      <td>['0920238243']</td>\n",
       "      <td>['', '', '', '', '', '']</td>\n",
       "      <td>[Skechers, Out, door, taupe, Size, 40,41,42,43...</td>\n",
       "      <td>Skechers Out door taupe Size 40,41,42,43 Price...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Skechers Out door taupe Size 40,41,42,43 Price...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    channel  message_id                       date  \\\n",
       "245  ethio_brand_collection        6081  2025-05-17T05:44:59+00:00   \n",
       "190       meneshayeofficial         951  2025-03-02T11:19:30+00:00   \n",
       "236  ethio_brand_collection        6090  2025-05-28T09:05:44+00:00   \n",
       "\n",
       "                                                  text   phone_numbers  \\\n",
       "245  Skechers Quantum flex \\nsize 40,41,42,43\\nPric...  ['0920238243']   \n",
       "190  áˆˆá‰ áˆˆáŒ  áˆ˜áˆ¨áŒƒ áŠ¨á‰³á‰½ á‹«áˆˆá‹áŠ• áˆ›áˆµáˆáŠ•áŒ áˆªá‹« á‹­áŒ«áŠ‘\\n\\nhttps://menes...              []   \n",
       "236  Skechers Out door taupe \\nSize 40,41,42,43\\nPr...  ['0920238243']   \n",
       "\n",
       "                       prices  \\\n",
       "245  ['', '', '', '', '', '']   \n",
       "190                        []   \n",
       "236  ['', '', '', '', '', '']   \n",
       "\n",
       "                                                tokens  \\\n",
       "245  [Skechers, Quantum, flex, size, 40,41,42,43, P...   \n",
       "190               [áˆˆá‰ áˆˆáŒ , áˆ˜áˆ¨áŒƒ, áŠ¨á‰³á‰½, á‹«áˆˆá‹áŠ•, áˆ›áˆµáˆáŠ•áŒ áˆªá‹«, á‹­áŒ«áŠ‘]   \n",
       "236  [Skechers, Out, door, taupe, Size, 40,41,42,43...   \n",
       "\n",
       "                                            clean_text  contains_phone  \\\n",
       "245  Skechers Quantum flex size 40,41,42,43 Price 3...            True   \n",
       "190                      áˆˆá‰ áˆˆáŒ  áˆ˜áˆ¨áŒƒ áŠ¨á‰³á‰½ á‹«áˆˆá‹áŠ• áˆ›áˆµáˆáŠ•áŒ áˆªá‹« á‹­áŒ«áŠ‘           False   \n",
       "236  Skechers Out door taupe Size 40,41,42,43 Price...            True   \n",
       "\n",
       "     contains_price                                            content  \n",
       "245            True  Skechers Quantum flex size 40,41,42,43 Price 3...  \n",
       "190           False                      áˆˆá‰ áˆˆáŒ  áˆ˜áˆ¨áŒƒ áŠ¨á‰³á‰½ á‹«áˆˆá‹áŠ• áˆ›áˆµáˆáŠ•áŒ áˆªá‹« á‹­áŒ«áŠ‘  \n",
       "236            True  Skechers Out door taupe Size 40,41,42,43 Price...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "865b9a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty prices lists: 0\n"
     ]
    }
   ],
   "source": [
    "empty_prices_count = df['prices'].apply(lambda x: isinstance(x, list) and len(x) == 0).sum()\n",
    "print(f\"Empty prices lists: {empty_prices_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a26cb",
   "metadata": {},
   "source": [
    "# problem\n",
    "price column has no exact price what to do\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
